{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#/root/.cache/huggingface/ models and dataset path"
      ],
      "metadata": {
        "id": "eEkHlH0vQm85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Log in to Hugging Face\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "DlgCBAjFPVvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "50Mx5-hh6ofB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json -O /content/alpaca_data.json\n"
      ],
      "metadata": {
        "id": "t4OJCQ1DQ31Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub pyarrow"
      ],
      "metadata": {
        "id": "EfqjI80CcS-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import json\n",
        "import random\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "save_dir = \"/content/drive/MyDrive/datasets\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "parquet_file = hf_hub_download(\n",
        "    repo_id=\"allenai/tulu-v2-sft-mixture\",\n",
        "    filename=\"data/train-00000-of-00003-99ee8754042a69f6.parquet\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "\n",
        "table = pq.read_table(parquet_file)\n",
        "df = table.to_pandas()\n",
        "print(f\"Uploaded {df.shape[0]} sample\")\n",
        "\n",
        "\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "tulu_train = df.iloc[:5000].to_dict(orient=\"records\")\n",
        "tulu_test = df.iloc[5000:7000].to_dict(orient=\"records\")\n",
        "\n",
        "# Save\n",
        "with open(f\"{save_dir}/tulu_v2_train.jsonl\", \"w\") as f:\n",
        "    for row in tulu_train:\n",
        "        f.write(json.dumps(row,default=str) + \"\\n\")\n",
        "\n",
        "with open(f\"{save_dir}/tulu_v2_test.jsonl\", \"w\") as f:\n",
        "    for row in tulu_test:\n",
        "        f.write(json.dumps(row,default=str) + \"\\n\")\n",
        "\n",
        "print(\"saved Tulu dataset.\")\n"
      ],
      "metadata": {
        "id": "QS9Wz1SDVyjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "# Drive mount\n",
        "drive.mount('/content/drive')\n",
        "save_dir = \"/content/drive/MyDrive/datasets\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Parqeut ultrachat\n",
        "train_parquet_path = hf_hub_download(\n",
        "    repo_id=\"HuggingFaceH4/ultrachat_200k\",\n",
        "    filename=\"data/train_sft-00000-of-00003-a3ecf92756993583.parquet\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "\n",
        "test_parquet_path = hf_hub_download(\n",
        "    repo_id=\"HuggingFaceH4/ultrachat_200k\",\n",
        "    filename=\"data/test_sft-00000-of-00001-f7dfac4afe5b93f4.parquet\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "\n",
        "train_df = pq.read_table(train_parquet_path).to_pandas()\n",
        "test_df = pq.read_table(test_parquet_path).to_pandas()\n",
        "print(f\" Train sample count: {train_df.shape[0]}\")\n",
        "print(f\" Test sample count : {test_df.shape[0]}\")\n",
        "\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "ultra_train = train_df.iloc[:5000].to_dict(orient=\"records\")\n",
        "ultra_test = test_df.iloc[:2000].to_dict(orient=\"records\")\n",
        "\n",
        "\n",
        "def to_serializable(obj):\n",
        "    if isinstance(obj, (bytes, bytearray)):\n",
        "        return obj.decode(\"utf-8\", errors=\"ignore\")\n",
        "    elif hasattr(obj, 'tolist'):\n",
        "        return obj.tolist()\n",
        "    return str(obj)\n",
        "\n",
        "\n",
        "with open(f\"{save_dir}/ultrachat_train.jsonl\", \"w\") as f:\n",
        "    for row in ultra_train:\n",
        "        f.write(json.dumps(row, default=to_serializable) + \"\\n\")\n",
        "\n",
        "with open(f\"{save_dir}/ultrachat_test.jsonl\", \"w\") as f:\n",
        "    for row in ultra_test:\n",
        "        f.write(json.dumps(row, default=to_serializable) + \"\\n\")\n",
        "\n",
        "print(\"Ultrachat saved\")\n"
      ],
      "metadata": {
        "id": "E0bj87VVWQ49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Drive mount\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/datasets\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Alpaca JSON\n",
        "with open(f\"{save_dir}/alpaca_data.json\", \"r\") as f:\n",
        "    alpaca_data = json.load(f)\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(alpaca_data)\n",
        "alpaca_train = alpaca_data[:5000]\n",
        "alpaca_test = alpaca_data[5000:7000]\n",
        "\n",
        "# SAve\n",
        "with open(f\"{save_dir}/alpaca_train.jsonl\", \"w\") as f:\n",
        "    for row in alpaca_train:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "with open(f\"{save_dir}/alpaca_test.jsonl\", \"w\") as f:\n",
        "    for row in alpaca_test:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(\"Alpace saved\")\n"
      ],
      "metadata": {
        "id": "wpT5-33VdgVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "def read_jsonl(path):\n",
        "    return [json.loads(line) for line in open(path, 'r', encoding='utf-8')]\n",
        "\n",
        "def format_to_prompt(example):\n",
        "    instruction = example.get(\"instruction\", \"\").strip()\n",
        "    input_text = example.get(\"input\", \"\").strip()\n",
        "    output = example.get(\"output\", \"\").strip()\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
        "    }\n",
        "\n",
        "def process_and_save(input_path, output_path):\n",
        "    data = read_jsonl(input_path)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for example in data:\n",
        "            formatted = format_to_prompt(example)\n",
        "            f.write(json.dumps(formatted, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"Saved formatted JSONL to {output_path}\")\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/datasets\"\n",
        "\n",
        "process_and_save(f\"{base_path}/alpaca_train.jsonl\", f\"{base_path}/alpaca_train_formatted.jsonl\")\n",
        "process_and_save(f\"{base_path}/alpaca_test.jsonl\", f\"{base_path}/alpaca_test_formatted.jsonl\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8I6jxA03e1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "path = \"/content/drive/MyDrive/datasets/tulu_v2_train.jsonl\"\n",
        "#path = \"/content/drive/MyDrive/datasets/ultrachat_train.jsonl\"\n",
        "\n",
        "with open(path, \"r\") as f:\n",
        "    for i in range(3):\n",
        "        line = f.readline()\n",
        "        if line:\n",
        "            example = json.loads(line)\n",
        "            print(f\"\\n--- Sample {i+1} ---\")\n",
        "            print(type(example))\n",
        "            for key, value in example.items():\n",
        "                print(f\"{type(key)}:\")\n",
        "                print(f\" {type(value)}\")\n",
        "                print(f\"{key}:{value}\")\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "kZ_Jx-IriGom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ast\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive mount\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#PAth\n",
        "input_path = \"/content/drive/MyDrive/datasets/tulu_v2_train.jsonl\"\n",
        "output_path = \"/content/drive/MyDrive/datasets/tulu_v2_train_formatted.jsonl\"\n",
        "\n",
        "# parsing\n",
        "def safe_parse_messages(messages_str):\n",
        "    try:\n",
        "        # added comma with tulu data\n",
        "        fixed = re.sub(r\"}\\s*{\", \"}, {\", messages_str.strip())\n",
        "        return ast.literal_eval(fixed)\n",
        "    except Exception as e:\n",
        "        print(f\" not parsed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def convert_message_to_instruct_format(messages_str):\n",
        "    messages = safe_parse_messages(messages_str)\n",
        "    if not messages:\n",
        "        return None\n",
        "\n",
        "    instruction = ''\n",
        "    input_part = ''\n",
        "    response = ''\n",
        "\n",
        "    for message in messages:\n",
        "        role = message.get('role')\n",
        "        content = message.get('content', '').strip()\n",
        "\n",
        "        if role == 'user':\n",
        "\n",
        "            if \"Translate to French\" in content:\n",
        "                instruction = \"Translate to French\"\n",
        "                match = re.search(r\"Translate to French:\\s*(.*?)\\n+Answer:\", content, re.DOTALL)\n",
        "                if match:\n",
        "                    input_part = match.group(1).strip()\n",
        "                else:\n",
        "                    input_part = content\n",
        "\n",
        "            # \"Question: ...\"\n",
        "            elif content.startswith(\"Question:\"):\n",
        "                lines = content.split('\\n')\n",
        "                for line in lines:\n",
        "                    if line.startswith('Question:'):\n",
        "                        instruction = line.replace('Question:', '').strip()\n",
        "                    elif not line.startswith(\"Sentence:\"):\n",
        "                        input_part += line.strip() + \"\\n\"\n",
        "\n",
        "\n",
        "            else:\n",
        "                instruction = \"Answer the question with reasoning\"\n",
        "                input_part = content\n",
        "\n",
        "        elif role == 'assistant':\n",
        "            response = content\n",
        "\n",
        "    if not instruction:\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_part.strip()}\\n\\n### Response:\\n{response}\"\n",
        "    }\n",
        "\n",
        "\n",
        "count_total = 0\n",
        "count_written = 0\n",
        "\n",
        "with open(input_path, \"r\") as fin, open(output_path, \"w\") as fout:\n",
        "    for i, line in enumerate(fin):\n",
        "        count_total += 1\n",
        "        try:\n",
        "            example = json.loads(line)\n",
        "            formatted = convert_message_to_instruct_format(example[\"messages\"])\n",
        "            if formatted and formatted[\"text\"].strip():\n",
        "                fout.write(json.dumps(formatted) + \"\\n\")\n",
        "                count_written += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error {i+1}: {e}\")\n",
        "\n",
        "print(f\"\\n total sample: {count_total}\")\n",
        "print(f\"Formatting sample: {count_written}\")\n",
        "print(f\"Saving: {output_path}\")\n"
      ],
      "metadata": {
        "id": "u7UPfQ1V6LS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ast\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive mount\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# PAth\n",
        "input_path = \"/content/drive/MyDrive/datasets/tulu_v2_test.jsonl\"\n",
        "output_path = \"/content/drive/MyDrive/datasets/tulu_v2_test_formatted.jsonl\"\n",
        "\n",
        "\n",
        "def safe_parse_messages(messages_str):\n",
        "    try:\n",
        "\n",
        "        fixed = re.sub(r\"}\\s*{\", \"}, {\", messages_str.strip())\n",
        "        return ast.literal_eval(fixed)\n",
        "    except Exception as e:\n",
        "        print(f\"not parsed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def convert_message_to_instruct_format(messages_str):\n",
        "    messages = safe_parse_messages(messages_str)\n",
        "    if not messages:\n",
        "        return None\n",
        "\n",
        "    instruction = ''\n",
        "    input_part = ''\n",
        "    response = ''\n",
        "\n",
        "    for message in messages:\n",
        "        role = message.get('role')\n",
        "        content = message.get('content', '').strip()\n",
        "\n",
        "        if role == 'user':\n",
        "\n",
        "            if \"Translate to French\" in content:\n",
        "                instruction = \"Translate to French\"\n",
        "                match = re.search(r\"Translate to French:\\s*(.*?)\\n+Answer:\", content, re.DOTALL)\n",
        "                if match:\n",
        "                    input_part = match.group(1).strip()\n",
        "                else:\n",
        "                    input_part = content\n",
        "\n",
        "            # \"Question: ...\"\n",
        "            elif content.startswith(\"Question:\"):\n",
        "                lines = content.split('\\n')\n",
        "                for line in lines:\n",
        "                    if line.startswith('Question:'):\n",
        "                        instruction = line.replace('Question:', '').strip()\n",
        "                    elif not line.startswith(\"Sentence:\"):\n",
        "                        input_part += line.strip() + \"\\n\"\n",
        "\n",
        "\n",
        "            else:\n",
        "                instruction = \"Answer the question with reasoning\"\n",
        "                input_part = content\n",
        "\n",
        "        elif role == 'assistant':\n",
        "            response = content\n",
        "\n",
        "    if not instruction:\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_part.strip()}\\n\\n### Response:\\n{response}\"\n",
        "    }\n",
        "\n",
        "\n",
        "count_total = 0\n",
        "count_written = 0\n",
        "\n",
        "with open(input_path, \"r\") as fin, open(output_path, \"w\") as fout:\n",
        "    for i, line in enumerate(fin):\n",
        "        count_total += 1\n",
        "        try:\n",
        "            example = json.loads(line)\n",
        "            formatted = convert_message_to_instruct_format(example[\"messages\"])\n",
        "            if formatted and formatted[\"text\"].strip():\n",
        "                fout.write(json.dumps(formatted) + \"\\n\")\n",
        "                count_written += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error {i+1}: {e}\")\n",
        "\n",
        "print(f\"\\n total sample: {count_total}\")\n",
        "print(f\"Formatting sample: {count_written}\")\n",
        "print(f\"Saving: {output_path}\")\n"
      ],
      "metadata": {
        "id": "WHbS9X7a_kzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 \"/content/drive/MyDrive/datasets/tulu_v2_test_formatted.jsonl\"\n"
      ],
      "metadata": {
        "id": "2M6hQDQj_lXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/datasets/ultrachat_train.jsonl\"\n",
        "output_path = \"/content/drive/MyDrive/datasets/ultrachat_train_formatted.jsonl\"\n",
        "\n",
        "\n",
        "def convert_ultrachat_messages_to_instruct_format(messages):\n",
        "    if not isinstance(messages, list) or len(messages) < 2:\n",
        "        return None\n",
        "\n",
        "    instruction = \"\"\n",
        "    input_parts = []\n",
        "    last_response = \"\"\n",
        "\n",
        "    for i, message in enumerate(messages):\n",
        "        role = message.get(\"role\")\n",
        "        content = message.get(\"content\", \"\").strip()\n",
        "\n",
        "        if i == 0 and role == \"user\":\n",
        "            instruction = content\n",
        "        elif role == \"user\":\n",
        "            input_parts.append(content)\n",
        "        elif role == \"assistant\":\n",
        "            last_response = content\n",
        "\n",
        "    if not instruction or not last_response:\n",
        "        return None\n",
        "\n",
        "    input_text = \"\\n\".join(input_parts).strip()\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{last_response.strip()}\"\n",
        "    }\n",
        "\n",
        "\n",
        "count_total = 0\n",
        "count_written = 0\n",
        "\n",
        "with open(input_path, \"r\") as fin, open(output_path, \"w\") as fout:\n",
        "    for i, line in enumerate(fin):\n",
        "        count_total += 1\n",
        "        try:\n",
        "            example = json.loads(line)\n",
        "            formatted = convert_ultrachat_messages_to_instruct_format(example[\"messages\"])\n",
        "            if formatted and formatted[\"text\"].strip():\n",
        "                fout.write(json.dumps(formatted) + \"\\n\")\n",
        "                count_written += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error {i+1}: {e}\")\n",
        "\n",
        "print(f\"\\n total sample: {count_total}\")\n",
        "print(f\"Formatting sample: {count_written}\")\n",
        "print(f\"Saving: {output_path}\")"
      ],
      "metadata": {
        "id": "PcZ86FwUQ_r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/datasets/ultrachat_test.jsonl\"\n",
        "output_path = \"/content/drive/MyDrive/datasets/ultrachat_test_formatted.jsonl\"\n",
        "\n",
        "\n",
        "def convert_ultrachat_messages_to_instruct_format(messages):\n",
        "    if not isinstance(messages, list) or len(messages) < 2:\n",
        "        return None\n",
        "\n",
        "    instruction = \"\"\n",
        "    input_parts = []\n",
        "    last_response = \"\"\n",
        "\n",
        "    for i, message in enumerate(messages):\n",
        "        role = message.get(\"role\")\n",
        "        content = message.get(\"content\", \"\").strip()\n",
        "\n",
        "        if i == 0 and role == \"user\":\n",
        "            instruction = content\n",
        "        elif role == \"user\":\n",
        "            input_parts.append(content)\n",
        "        elif role == \"assistant\":\n",
        "            last_response = content\n",
        "\n",
        "    if not instruction or not last_response:\n",
        "        return None\n",
        "\n",
        "    input_text = \"\\n\".join(input_parts).strip()\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{last_response.strip()}\"\n",
        "    }\n",
        "\n",
        "\n",
        "count_total = 0\n",
        "count_written = 0\n",
        "\n",
        "with open(input_path, \"r\") as fin, open(output_path, \"w\") as fout:\n",
        "    for i, line in enumerate(fin):\n",
        "        count_total += 1\n",
        "        try:\n",
        "            example = json.loads(line)\n",
        "            formatted = convert_ultrachat_messages_to_instruct_format(example[\"messages\"])\n",
        "            if formatted and formatted[\"text\"].strip():\n",
        "                fout.write(json.dumps(formatted) + \"\\n\")\n",
        "                count_written += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error {i+1}: {e}\")\n",
        "\n",
        "print(f\"\\n total sample: {count_total}\")\n",
        "print(f\"Formatting sample: {count_written}\")\n",
        "print(f\"Saving: {output_path}\")"
      ],
      "metadata": {
        "id": "H7FThqUeRfdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 \"/content/drive/MyDrive/datasets/ultrachat_test_formatted.jsonl\"\n"
      ],
      "metadata": {
        "id": "ITMLLo7ERtGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_jsonl_files(input_files, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
        "        for file in input_files:\n",
        "            with open(file, 'r', encoding='utf-8') as in_f:\n",
        "                for line in in_f:\n",
        "                    out_f.write(line)\n",
        "    print(f\"Combined dataset saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "PuDsJGidf4DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/datasets\"\n",
        "\n",
        "\n",
        "combine_jsonl_files(\n",
        "    [\n",
        "        f\"{base_path}/alpaca_train_formatted.jsonl\",\n",
        "        f\"{base_path}/tulu_v2_train_formatted.jsonl\",\n",
        "        f\"{base_path}/ultrachat_train_formatted.jsonl\"\n",
        "    ],\n",
        "    f\"{base_path}/combined_train.jsonl\"\n",
        ")\n",
        "\n",
        "combine_jsonl_files(\n",
        "    [\n",
        "        f\"{base_path}/alpaca_test_formatted.jsonl\",\n",
        "        f\"{base_path}/tulu_v2_test_formatted.jsonl\",\n",
        "        f\"{base_path}/ultrachat_test_formatted.jsonl\"\n",
        "    ],\n",
        "    f\"{base_path}/combined_test.jsonl\"\n",
        ")"
      ],
      "metadata": {
        "id": "CWDwoms1hP6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l /content/drive/MyDrive/datasets/combined_test.jsonl"
      ],
      "metadata": {
        "id": "fWJCZL0giE05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l /content/drive/MyDrive/datasets/combined_train.jsonl"
      ],
      "metadata": {
        "id": "J_mFl1BKhZg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7q0ADE2USwlP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}